input:
  _target_: torchvision.transforms.v2.Compose
  transforms:
  - _target_: torchvision.transforms.v2.Resize
    size: [512, 512]
  - _target_: torchvision.transforms.v2.Grayscale

train:
  _target_: torchvision.transforms.v2.Compose
  transforms:
  - _target_: torchvision.transforms.v2.Resize
    size: [512, 512]
  - _target_: torchvision.transforms.v2.Grayscale
  - _target_: torchvision.transforms.v2.Normalize
    mean: [0.1307910680770874]
    std: [0.18631263077259064]

test:
  _target_: torchvision.transforms.v2.Compose
  transforms:
  - _target_: torchvision.transforms.v2.Resize
    size: [512, 512]
  - _target_: torchvision.transforms.v2.Grayscale
  - _target_: torchvision.transforms.v2.Normalize
    mean: [0.1307910680770874]
    std: [0.18631263077259064]

target:
  _target_: torchvision.transforms.v2.Compose
  transforms:
  - _target_: torchvision.transforms.v2.Resize
    size: [512, 512]