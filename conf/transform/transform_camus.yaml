input:
  _target_: torchvision.transforms.v2.Compose
  transforms:
  - _target_: torchvision.transforms.v2.Resize
    size: [512, 512]
  - _target_: torchvision.transforms.v2.Grayscale

train:
  _target_: torchvision.transforms.v2.Compose
  transforms:
  - _target_: torchvision.transforms.v2.Resize
    size: [512, 512]
  - _target_: torchvision.transforms.v2.Grayscale
  - _target_: torchvision.transforms.v2.Normalize
    mean: [0.257314532995224]
    std: [0.2964794635772705]

test:
  _target_: torchvision.transforms.v2.Compose
  transforms:
  - _target_: torchvision.transforms.v2.Resize
    size: [512, 512]
  - _target_: torchvision.transforms.v2.Grayscale
  - _target_: torchvision.transforms.v2.Normalize
    mean: [0.257314532995224]
    std: [0.2964794635772705]

target:
  _target_: torchvision.transforms.v2.Compose
  transforms:
  - _target_: torchvision.transforms.v2.Resize
    size: [512, 512]